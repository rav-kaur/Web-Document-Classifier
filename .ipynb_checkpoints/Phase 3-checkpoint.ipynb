{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "from sklearn.feature_extraction import text\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ravneet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ravneet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files(os.getcwd() + \"/Datasets\")\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    document = re.sub('(?<=[A-Za-z])(?=[A-Z][a-z])', ' ', str(X[sen]))\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(\"n \", \"\", document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    document = re.sub(r'(\\s)x\\w+', r'\\1', document)\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    \n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "X_train_counts = count_vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tf, y, test_size=0.2, random_state=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 4 0]\n",
      " [3 0 0]]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "count = 0\n",
    "path = os.getcwd() + \"/Test\"\n",
    "for dirpath, dirnames, filenames in os.walk(path):  # getcwd() for current work dir\n",
    "    file_names = filenames\n",
    "\n",
    "file_content = []\n",
    "for file_name in file_names:\n",
    "    if ('.txt' in file_name):\n",
    "        file_path = path+\"/\"+file_name\n",
    "        file = open(file_path)\n",
    "        txt = file.read()\n",
    "        file.close()\n",
    "\n",
    "        file_content.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(file_content)):\n",
    "    document = re.sub('(?<=[A-Za-z])(?=[A-Z][a-z])', ' ', str(file_content[sen]))\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(\"n \", \"\", document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    document = re.sub(r'(\\s)x\\w+', r'\\1', document)\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    \n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    new_documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = count_vect.transform(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_tf = tf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(X_new_tf.toarray())\n",
    "\n",
    "predictions = []\n",
    "for doc, category in zip(new_documents, predicted):\n",
    "    predictions.append(data.target_names[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = ['Internet of Things', 'Blockchain', 'Blockchain', 'Artificial Intelligence', 'Internet of Things', 'Artificial Intelligence','Artificial Intelligence','Artificial Intelligence','Blockchain', 'Internet of Things','Blockchain', 'Internet of Things']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prediction'] = predictions\n",
    "df['Expected'] = expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['File_names', 'Prediction', 'Expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_names</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Internet of Things (IoT): An Overview - In...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Internet of Things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The strategic business value of the blockchain...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Blockchain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explainer: What is a blockchain? | MIT Technol...</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Artificial Intelligence - Journal - Elsevier.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Internet Of Things (iot) - Gartner.txt</td>\n",
       "      <td>Internet of Things</td>\n",
       "      <td>Internet of Things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 Things AI Can Already Do for Your Company.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Artificial Intelligence and the Future of Huma...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Artificial intelligence is evolving all by its...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the Blockchain? Explaining the Tech Be...</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What Is the Internet of Things (IoT)? | Oracle...</td>\n",
       "      <td>Internet of Things</td>\n",
       "      <td>Internet of Things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What Is Blockchain? The Complete WIRED Guide |...</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Internet of Things Inc.: Artificial Intelligen...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Internet of Things</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_names  \\\n",
       "0   The Internet of Things (IoT): An Overview - In...   \n",
       "1   The strategic business value of the blockchain...   \n",
       "2   Explainer: What is a blockchain? | MIT Technol...   \n",
       "3    Artificial Intelligence - Journal - Elsevier.txt   \n",
       "4              Internet Of Things (iot) - Gartner.txt   \n",
       "5     3 Things AI Can Already Do for Your Company.txt   \n",
       "6   Artificial Intelligence and the Future of Huma...   \n",
       "7   Artificial intelligence is evolving all by its...   \n",
       "8   What is the Blockchain? Explaining the Tech Be...   \n",
       "9   What Is the Internet of Things (IoT)? | Oracle...   \n",
       "10  What Is Blockchain? The Complete WIRED Guide |...   \n",
       "11  Internet of Things Inc.: Artificial Intelligen...   \n",
       "\n",
       "                 Prediction                 Expected  \n",
       "0   Artificial Intelligence       Internet of Things  \n",
       "1   Artificial Intelligence               Blockchain  \n",
       "2                Blockchain               Blockchain  \n",
       "3   Artificial Intelligence  Artificial Intelligence  \n",
       "4        Internet of Things       Internet of Things  \n",
       "5   Artificial Intelligence  Artificial Intelligence  \n",
       "6   Artificial Intelligence  Artificial Intelligence  \n",
       "7   Artificial Intelligence  Artificial Intelligence  \n",
       "8                Blockchain               Blockchain  \n",
       "9        Internet of Things       Internet of Things  \n",
       "10               Blockchain               Blockchain  \n",
       "11  Artificial Intelligence       Internet of Things  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
